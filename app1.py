"""
Emotion-Aware AI Writing Assistant (Streamlit single-file app)

Features:
- Emotion detection using a Hugging Face emotion classification model
- Story continuation using distilgpt2
- Optional image generation using Stable Diffusion (diffusers)

Notes:
- Image generation requires Hugging Face authentication token (HF_TOKEN) and will run much faster on GPU.
- This example keeps things simple and focuses on clarity. Modify prompt templates, models, and UI to taste.

Run:
1) (Optional) create and activate a venv
2) pip install -U pip
3) pip install streamlit transformers diffusers accelerate safetensors torch torchvision pillow
   - If you have CUDA and want FP16 speedups, install the matching torch build from https://pytorch.org
4) Export HF_TOKEN environment variable if using Stable Diffusion:
   export HF_TOKEN="<your_hf_token>"
5) streamlit run emotion_aware_story_assistant_app.py

"""

import os
import io
import textwrap
from typing import List, Dict

import streamlit as st
from PIL import Image

# Transformers / Hugging Face
from transformers import pipeline

# Diffusers (optional)
try:
    from diffusers import StableDiffusionPipeline
    import torch
    DIFFUSERS_AVAILABLE = True
except Exception:
    DIFFUSERS_AVAILABLE = False

# -----------------------------
# Helper: load models (cached)
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_emotion_pipeline():
    """Load a small emotion classification model from Hugging Face."""
    # Model choice: small, accurate emotion classifier
    model_name = "j-hartmann/emotion-english-distilroberta-base"
    try:
        emo_pipe = pipeline("text-classification", model=model_name, return_all_scores=True)
    except Exception as e:
        st.error(f"Failed to load emotion model {model_name}: {e}")
        raise
    return emo_pipe

@st.cache_resource(show_spinner=False)
def load_textgen_pipeline():
    """Load a lightweight text generation model."""
    gen_model = "distilgpt2"
    try:
        gen_pipe = pipeline("text-generation", model=gen_model, device_map="auto")
    except Exception:
        # fallback to CPU if no GPU
        gen_pipe = pipeline("text-generation", model=gen_model)
    return gen_pipe

@st.cache_resource(show_spinner=False)
def load_sd_pipeline(hf_token: str = None):
    """Load stable diffusion pipeline if diffusers available and user provided a token.
    Returns None if unavailable.
    """
    if not DIFFUSERS_AVAILABLE:
        return None
    model_id = "runwayml/stable-diffusion-v1-5"
    try:
        # prefer GPU if available
        if torch.cuda.is_available():
            pipe = StableDiffusionPipeline.from_pretrained(model_id, use_safetensors=True, torch_dtype=torch.float16, revision="fp16", use_auth_token=hf_token)
            pipe = pipe.to("cuda")
        else:
            pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=hf_token)
    except Exception as e:
        st.warning("Stable Diffusion pipeline failed to load. Image generation will be disabled.\n" + str(e))
        return None
    return pipe

# -----------------------------
# Core logic
# -----------------------------

def detect_emotion(emo_pipe, text: str) -> List[Dict]:
    """Return ranked list of emotions with scores."""
    results = emo_pipe(text)
    # results is a list of lists (batch). We passed single text so results[0]
    if isinstance(results, list) and len(results) and isinstance(results[0], list):
        return sorted(results[0], key=lambda x: x["score"], reverse=True)
    return results


def build_continuation_prompt(original_text: str, top_emotion: str) -> str:
    """Create a prompt for the text-generation model that nudges it to continue matching the emotion."""
    prompt = textwrap.dedent(f"""
    Continue the following story in the same voice and maintain the emotional tone of '{top_emotion}'.

    Story start:
    {original_text}

    Continue:
    """)
    return prompt


def generate_continuation(gen_pipe, prompt: str, max_new_tokens: int = 120) -> str:
    # distilgpt2's tokenizer counts input length as well. Keep a sensible max length.
    try:
        out = gen_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.9, top_k=50, top_p=0.95)
        # pipeline returns list of dicts with 'generated_text'
        return out[0]["generated_text"][len(prompt):].strip()
    except TypeError:
        # older transformers versions use 'max_length' instead
        out = gen_pipe(prompt, max_length= len(prompt.split()) + max_new_tokens, do_sample=True, temperature=0.9)
        return out[0]["generated_text"][len(prompt):].strip()


def generate_image(sd_pipe, prompt: str, guidance_scale: float = 7.5, num_inference_steps: int = 30):
    """Return a PIL.Image generated by Stable Diffusion."""
    if sd_pipe is None:
        raise RuntimeError("Stable Diffusion pipeline not available")
    with torch.autocast("cuda") if torch.cuda.is_available() else nullcontext():
        image = sd_pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]
    return image

# tiny nullcontext for CPU path
from contextlib import nullcontext

# -----------------------------
# Streamlit App UI
# -----------------------------

st.set_page_config(page_title="Emotion-Aware Story Assistant", layout="wide")
st.title("üé≠ Emotion-Aware AI Writing Assistant ‚Äî Dynamic Storytelling")

st.sidebar.header("Settings")
show_images = st.sidebar.checkbox("Enable image generation (requires HF token & diffusers)", value=False)
hf_token = os.environ.get("HF_TOKEN") if show_images else None
if show_images and not hf_token:
    st.sidebar.warning("Set HF_TOKEN environment variable to enable image generation.")

with st.spinner("Loading models..."):
    emo_pipe = load_emotion_pipeline()
    gen_pipe = load_textgen_pipeline()
    sd_pipe = load_sd_pipeline(hf_token) if show_images else None

st.sidebar.markdown("---")
max_new_tokens = st.sidebar.slider("Max new tokens for continuation", min_value=50, max_value=400, value=140)

# Main UI
st.subheader("Enter your story or scene snippet")
user_text = st.text_area("Story input", height=200)

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("Analyze Emotion & Generate Continuation"):
        if not user_text.strip():
            st.warning("Please type a short story fragment first.")
        else:
            with st.spinner("Detecting emotion..."):
                emo_results = detect_emotion(emo_pipe, user_text)
                top = emo_results[0]
                st.markdown(f"**Top emotion:** {top['label']} ‚Äî score: {top['score']:.2f}")
                # show full distribution
                st.write(emo_results)

            with st.spinner("Generating continuation..."):
                prompt = build_continuation_prompt(user_text, top['label'])
                continuation = generate_continuation(gen_pipe, prompt, max_new_tokens=max_new_tokens)
                st.markdown("### Continuation")
                st.write(continuation)

            if show_images and sd_pipe is not None:
                img_prompt = f"Cinematic, highly detailed illustration reflecting the emotion '{top['label']}' for this scene: {user_text} Continue: {continuation} --ar 16:9"
                st.markdown("### Generated Image")
                try:
                    with st.spinner("Generating image (this may take a while)..."):
                        image = generate_image(sd_pipe, img_prompt)
                        st.image(image, use_column_width=True)
                except Exception as e:
                    st.error(f"Image generation failed: {e}")

with col2:
    st.markdown("### Quick prompts & controls")
    st.markdown("- Tips: keep the input 1-5 short paragraphs for best continuations.\n- Use the slider to let the generator produce longer/shorter continuations.")
    st.markdown("---")
    st.markdown("### Example inputs")
    if st.button("Load example: melancholic swing"):
        st.session_state['example'] = "She stood under the oak, watching the empty swing sway in the autumn breeze, thinking of laughter that would never return."
        st.experimental_rerun()
    if 'example' in st.session_state:
        st.text_area("Story input", value=st.session_state['example'], key='story_example')

st.markdown("---")
st.markdown("Built with ‚ô•Ô∏è ‚Äî adjust models & UI in the file to customize behavior.")

# End of app
